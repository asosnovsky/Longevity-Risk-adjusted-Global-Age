---
title: "Optimization Issues"
geometry: margin=1cm
output: 
  pdf_document: 
    fig_width: 10
    toc: yes
---

```{r include=FALSE}
# Set the seed for any future randomization needs
set.seed(0)

# Load used libraries
library(readr)
library(tidyverse)
library(scales)
library(knitr)

# Util methods
compute_table1 <- function(stage1_model) stage1_model %>% 
  left_join(countries, by=c("Country")) %>% 
  mutate( g = g %>% percent(accuracy = 0.001) ) %>% 
  mutate( H = H*10^5, l_m = l_m*10^5 ) %>% 
  mutate_if(is.numeric, ~round(., 3)) %>% arrange(Gender) %>% 
  select(Gender, `Country Name`, l_m, H, g, lnh, m, b) %>% 
  rename( `H*10^5` = H, `l_m*10^5` = l_m )

# Read in the merged dataset for 2011
dataset = read_csv("../../data/01_processed/2011_qx_data.csv") %>% 
  # Remove Iceland as its too much of an outlier
  filter(
    !(Country %in% c("ISL"))
  )
countries = read_csv("../../data/01_processed/country_codes.csv") %>% 
  rename(`Country Name`=CountryName, Country=CountryCode)
```

# Introduction

I managed to use the "L-BFGS-B" algorithmn (Byrd et. al. (1995)), which is implemented in base-R `optim` function. The algorithmn utilizes the gradient of the function and some box-constraint to optimize the function to a minimum. However, I found that the optimization is relient on the constraint provided. Additionally, I found it a bit hard to trust the result given the ouput of the stage-2 model ($lnh = Lg-x^{*}$). The optimization relies on the boundaries for $H$ to be well-defined, as larger upper-boundaries of $H$ causes $\lambda_m \rightarrow 0$.

In order to run further analysis, I decided to learn the function emperically and found that the best fit appears to occur when $\min_x \tilde{q_x} \approx \lambda$.

\newpage

# Optimization

## 1. Target Functions
First I had to define the target functions as curried-functions (functions that return functions).

```{r}
# Target Functions
U = function(qtx, Age) function(par) {
  # extract parameters
  l = par[1]; H = par[2]; g = par[3];
  # evaluate function
  sum( ( qtx - l - H*exp(g*Age) )^2 ) / (max(Age) - min(Age))
}
dU = function(qtx, Age) function(par) {
  # extract parameters
  l = par[1]; H = par[2]; g = par[3];
  n = max(Age) - min(Age)
  # describe the "middile-part of the summation"
  inner = qtx - l - H*exp(g*Age)
  # compute each derivative
  dUdl = sum( inner*(-2)/n )
  dUdH = sum( inner*(-2)*exp(g*Age) / n )
  dUdg = sum( inner*(-2)*H*Age*g*exp(g*Age) / n )
  # return
  c(dUdl, dUdH, dUdg)
}
```

## 2. Optimization

Next I define the method that we will use.
```{r}
fit_model = function(qtx, Age) optim( 
  method = 'L-BFGS-B',
  # initial guess
  par=c( 1E-5, 1E-5, 1E-2 ), 
  # target functions
  fn=U(qtx, Age), gr=dU(qtx, Age),
  # boundaries
  lower = c(0, 0, 0), upper = c(min(qtx), 1E-4, 1)
)
```

## 3. Run Optimzation

```{r}
dataset %>%
  filter(between(Age, 35, 95)) %>% 
  mutate( qtx = -log( 1-qx ) ) %>% 
  group_by(Gender, Country) %>% nest %>% mutate(
    # compute the model for each country/gender group
    model = map(data, ~fit_model(.$qtx, .$Age)$par ),
    # extract parameters
    l_m = map_dbl(model, ~.[1]),
    H = map_dbl(model, ~.[2]),
    g = map_dbl(model, ~.[3]),
    h = g*H/(exp(g)-1),
    lnh = log(h),
    m = (log(g) - lnh)/g,
    b = 1/g,
    U = map2_dbl(data, model, ~U(.x$qtx, .x$Age)(.y))
  ) -> Stage1_model
```

```{r include=FALSE}
Stage1_model %>% 
  compute_table1 -> table1
```

\newpage
### Table1a: Fit on Males
```{r echo=FALSE}
table1 %>% filter(Gender=="Male") %>% select(-Gender) %>% knitr::kable()
```

\newpage
### Table1b: Fit on Females
```{r echo=FALSE}
table1 %>% filter(Gender=="Female") %>% select(-Gender) %>% knitr::kable()
```

\newpage

## 4. Concerns
The above estimation used a number of constraints that I acquired emperically by looking at the linear regression fit where $\lambda_m = 0$. The constraints for $\lambda_m$ are based on the original results of the paper you've sent me.

$$
  0 \le \lambda_m \le \min{\tilde{q_x}}
$$

$$
  0 \le H \le 5*10^{-4}
$$

$$
  0 \le g \le 1
$$

A slight tweak to the constraints (say increasing the upper-boundary of $H$ by a factor of $10$ result in drastically different results).


In the figure below, I have utilized different values for the upper-boundary of $H$, ranging from $10^{-5}$ to 1. Observe how the value of $g$ and $H$ become more relevant for higher upper boundaries of $H$ while $\lambda_m$ approach zero.

```{r echo=FALSE, fig.height=7, fig.width=10}
# Function to minimize the output
fit_model = function(
  upper, 
  lower = c(0,0,0),
  control=list(fnscale = c(1, 1, 1))
) function(qtx, Age) optim( 
  par=c( min(qtx)/2, 1E-5, 1E-2 ), # <- these starting points were chosen to avoid local-minimas
  fn=U(qtx, Age), 
  gr=dU(qtx, Age),
  lower = lower,
  upper = upper, method = 'L-BFGS-B',
  control = control
)

compute_stage1 <- function(upper, lower=c(0,0,0), control=list(fnscale = c(1, 1, 1))) dataset %>%
  filter(between(Age, 35, 95)) %>% 
  mutate(
    qtx = -log( 1-qx )
  ) %>% 
  group_by(Gender, Country) %>% nest %>% mutate(
    model = map(data, ~fit_model(
      upper = upper(.$qtx),
      lower = lower,
      control = control
    )(.$qtx, .$Age)$par ),
    l = map_dbl(model, ~.[1]),
    H = map_dbl(model, ~.[2]),
    g = map_dbl(model, ~.[3]),
    h = g*H/(exp(g)-1),
    lnh = log(h),
    m = (log(g) - lnh)/g,
    b = 1/g,
    U = map2_dbl(data, model, ~U(.x$qtx, .x$Age)(.y))
  ) %>% select(Gender, Country, l, H, g, lnh, U)

lapply(10^-(0:5), function(uH) compute_stage1(
  upper = function(qtx) c(min(qtx), uH, 1),
  control = c(1E5, 1, 1)
) %>% mutate(upperH = uH)) %>% reduce(bind_rows) -> tmp

tmp %>%  
  gather(param, value, -c(Gender, Country, upperH, U)) %>% 
  ggplot() + facet_grid(param~upperH, scales = 'free_y') + 
  geom_point(aes(U,value))
```

Please note that in addition to that, the values of $\ln{h}$ appear to have a much smaller variance for smaller boundaries of $H$, we effectively have a trade-off between $\lambda_m$ and the accuracy of $\ln{h}$. In the figure below I show-case how the boundaries impact the variance of $\ln{h}$ and the relationship between $g$ and $\ln{h}$.

```{r echo=F, fig.height=7, fig.width=10}
tmp %>% ggplot() + facet_grid(upperH~., scales = 'fixed') + 
  geom_point(aes(lnh, g))
```

# Emperical Approximation

In this approach (which is much more resource intensive), I attempt to find the best minima using a grid-search for the parameters. In essence, this approach is a brute-force approach to identify the parameters.

Below is the methodology
```{r}
grid_search <- function(qtx, Age) {
  u = U(qtx, Age);  du = dU(qtx, Age);
  # generate the grid
  lapply(
    seq(0,0.5,l=50), function(g) lapply(
      seq(0,1E-2,l=100), function(H) lapply(
        seq(0,min(qtx),l=50), function(l) tibble(
          # Compute values
          g=g,l=l,H=H, 
          U=u(c(g,H,l)),
          dU=du(c(g,H,l))
        )
      ) %>% reduce(bind_rows)
    ) %>% reduce(bind_rows)
  ) %>% reduce(bind_rows) %>% 
  # Compute the rest of the values and format l and H
  mutate(
    l = l*1E5, H = H*1E5,
    h = g*H/(exp(g)-1),
    lnh = log(h),
    m = (log(g) - lnh)/g,
    b = 1/g
  )
}
```

```{r, echo=F}
emp_dataset = dataset %>%
  filter(between(Age, 35, 95)) %>% 
  mutate( qtx = -log( 1-qx ) ) %>% 
  group_by(Gender, Country) %>% nest %>% 
  unite(id, Country, Gender) %>% 
  spread(id, data)
```

Nest I compute the result for Russian, Ukrainian and Canadian males.

```{r eval=FALSE, include=T}
data_russ_male = emp_dataset$RUS_Male[[1]]
data_ukr_male = emp_dataset$UKR_Male[[1]]
data_can_male = emp_dataset$CAN_Male[[1]]

emp_rus = with(data_russ_male, { grid_search(qtx, Age) })
emp_ukr = with(data_ukr_male, { grid_search(qtx, Age) })
emp_can = with(data_can_male, { grid_search(qtx, Age) })
```

```{r include=FALSE}
data_russ_male = emp_dataset$RUS_Male[[1]]
data_ukr_male = emp_dataset$UKR_Male[[1]]
data_can_male = emp_dataset$CAN_Male[[1]]
emp_rus <- read_csv("./emp_rus.csv")
emp_ukr <- read_csv("./emp_ukr.csv")
emp_can <- read_csv("./emp_can.csv")
```

Then I output the parameters for the lowest values of $\Upsilon$. Note that the values of $\lambda_m$ are always equal to the minima of $\tilde{q_x}$.

```{r}
emp_rus %>% filter( U == min(U) ) %>% distinct(g,l,H) %>% 
  mutate( `min(qx)` = min(data_russ_male$qtx)*10^5 ) %>% kable()
emp_ukr %>% filter( U == min(U) ) %>% distinct(g,l,H) %>%
  mutate( `min(qx)` = min(data_ukr_male$qtx)*10^5 ) %>% kable()
emp_can %>% filter( U == min(U) ) %>% distinct(g,l,H) %>%
  mutate( `min(qx)` = min(data_can_male$qtx)*10^5 ) %>% kable()
```

Finally, I also looked at the relationship between the variables and the lowest $\Upsilon$ they generated.

```{r, fig.height=2.5}
emp_rus %>% 
  group_by(l) %>% summarise(U = min(U)) %>% 
  ggplot() + geom_point(aes(l, U)) +
  geom_vline(xintercept=min(data_russ_male$qtx)*10^5)
```
Please note that the straight-line represents the minium of $\tilde{q_x}$.

```{r, fig.height=2.5}
emp_rus %>% 
  group_by(g) %>% summarise(U = min(U)) %>% 
  ggplot() + geom_point(aes(g, U))
  
emp_rus %>% 
  group_by(H) %>% summarise(U = min(U)) %>% 
  ggplot() + geom_point(aes(H, U))
```

# Conclusion
First, please be aware that I have attempted a number of other approaches as well. Among these,

- Using the residual values of the zero-makeham model as an estimate for $\lambda_m$.
- Scalling $lamnda_m$ by a factor of $10^5$ and running it against Newton-method with unconstraint parameters

These methods consistently over-estimated or under-estimated the variables.


We may need to generate the values here using an analytical approach. However, in the meantime, please consider the previous approach where we manually tried various models with varying lambda's and then picked the best one in accordance to the model's fit. As mentioned in a previous email, the values of lambda appeared large during our skype call due to the display function multiplying the result by 10^6 instead of 10^5. 

https://github.com/asosnovsky/Mortality-Adj-Bio-Age-Around-the-world/blob/master/data/02_paper_tables/table_1_Male.csv