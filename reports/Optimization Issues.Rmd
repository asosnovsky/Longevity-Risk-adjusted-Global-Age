---
title: "Optimization Issues"
output:
  pdf_document: default
  html_document: default
---

Starting with the following,

$$
  \Psi(\lambda, h, g) = \frac{1}{x_{max}-x_{min}} ( \sum_x [\ln\frac{1}{1-q_x} - \frac{h}{g}e^{gx}(e^g-1)-\lambda ] )^2
$$

For simplicity, Set 
$$
  R = x_{max}-x_{min}  \ , \ \ 
  Q = \sum_x \ln\frac{1}{1-q_x} \ , \ \ 
  H = \frac{h}{g}(e^g-1)
$$

So,
$$
  \Psi(\lambda, h, g) = \frac{1}{R} ( Q - R\lambda - H\sum_x e^{gx} )^2
$$

Note that $\lambda$ here has minimal interaction with any of the obesrvations. It's actually quite trivial to show that to minimize $\Psi$ on $\lambda$ we set $\lambda$ to minimum of its range (i.e. $\lambda \rightarrow 0$). This is infact what we get when we run optimization functions in R or visualize $\Psi$ through brute force.

Additionally, if we took the exponent of equation (10) in your paper:

$$
\ln(\frac{1}{1-q_x}) = \frac{h}{g}(e^g-1)e^{gx} + \lambda
$$

We get an expression analgous to:
$$
y = ae^{bx} + c
$$
This represent the model that we would need to solve, and unfourtinately the gradient of this expression at $c=\lambda$ is just $1$, making it impossible to optimize.
